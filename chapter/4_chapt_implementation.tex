\graphicspath{{./figures/}}
\chapter{Implementation}
This chapter focuses on the overall project's implementation.
It mainly covers the four parts hardware assembly, long jump analysis,
drone control and their consolidation into one \ac{GUI}.

\section{Long-jump analysis}\label{sec:4_analysis_software}
In order to analyze recorded long jump videos a ground station software is
developed.
Generally, the analysis is performed regarding the following set of
parameters:
\begin{itemize}
    \item left / right knee angle
    \item left / right arm angle
    \item takeoff angle
    \item left / right foot position
    \item hip height
\end{itemize}
These parameters are tracked over the whole jump, beginning with the run-up
throughout the takeoff until the landing.\\
\autoref{fig:4_long_jump_sketch} shows a detailed overview over the video data
that can be analyzed by the software.\\
As the takeoff is (one of) the most important phases in a long jump, it is
important to be able to detect the takeoff in a video.
Such a takeoff detection is developed alongside the above-mentioned parameter 
detection- and calculation.\\
This section will first introduce the body key point detection process that
is basis for all following calculations.
Afterwards, an algorithm for an automatic takeoff frame detection based on a
video input is implemented.
\begin{figure}[!h]
    \centering
    \includegraphics[scale=0.6]{long_jump_sketch.pdf}
    \caption[Long jump parameter overview]{General long jump overview.\\
    Parameters that can be analyzed are marked \textcolor{red}{red}.\\
    \textcolor{cyan}{$TP_0$} and \textcolor{cyan}{$TP_1$} denote phase
    transition points.}
    \label{fig:4_long_jump_sketch}
\end{figure}
\FloatBarrier

\subsection{Video processing}\label{subsec:4_body_keypoint_detection}
After a video has been recorded, several processing steps need to be performed
to calculate the parameters shown in \autoref{fig:4_long_jump_sketch}.
All calculations are performed for each input video frame respectively.
In the first processing stage, multiple filters can be applied to each frame.
The body key point detection is then performed on the filtered output. 
Based on these key points, the jumping parameters can be calculated which are
then in a last stage saved to a hdf5 file as well as used for the automatic
takeoff frame detection.\\
The whole process is visualized below in \autoref{fig:4_video_pipeline}.
\begin{figure}[!h]
    \centering
    \includegraphics[scale=0.54]{video_pipeline.pdf}
    \caption[Video processing pipeline]{Video processing pipeline.\\
    Stages, that each video frame passes through.}
    \label{fig:4_video_pipeline}
\end{figure}
\FloatBarrier
\noindent In the following subsections each of the shown processing stages
will be explained in detail.

\subsubsection*{Pre-processing via filter application}\label{subsubsec:4_filter_processing}
The first stage is meant to prepare the incoming video frames to get the best
possible result from the following pose detection stage.
Thus, this stage will in the following be referred to as \textit{pre
processing stage}.\\
To get an accurate pose detection result it is important to have sharp edges
around the athletes' body and an overall low noise level.
This can be achieved by applying different filters to each frame.\\
Three of the most commonly used filters are high-pass (sharpening), low-pass
(blurring) and edge preserving low-pass filters, also known as bilateral
filters.
As the best choice depends on the video input and recording conditions,
all of them are implemented and can be mixed with each other.\\
The low- and high pass filter operations can be expressed as 2D spatial
convolutions of an input image \textbf{I} of size $(x_i,y_i)$ and a (quadratic)
filter kernel \textbf{K} of size $(x_k, y_k)$.
\begin{equation}
    C(i, j) = \sum_{m=-a}^{a} \sum_{n=-b}^{b} K(m, n) \cdot I(i - m, j - n)
\end{equation}   
where C is the filtered output frame, $a = \lfloor \frac{x_k}{2} \rfloor$ and
$b = \lfloor \frac{x_i}{2} \rfloor$.
This method is also known as moving window filtering and is supported by
opencv using the \texttt{filter2D()} method.\\
The low pass filter especially helps to reduce noise in an input video frame.
Because of its' blurring character, the edges around the athletes' body lose
their details as well.
However, especially in low light conditions, this can be a good first approach
to get better results from the key point detector stage.
The high pass filter on the other hand is not suitable for videos that contain
noisy frames.
The the noise would be amplified as well as the edges which leads to
a worse body key point detection performance.
However, in high quality video recordings, the high pass filter helps to
enhance the overall sharpness and therefore especially the edges around the
athletes' body.\\

\noindent To reduce the overall noise level in a video frame while preserving
edges, a third filter, namely the \textit{bilateral filter}, is implemented.
It is defined by the following equation:
\begin{equation}
    C(I)_p = \frac{1}{W_p}\sum_{q \in S}^{} {G_\sigma}_s(\lVert p - q \rVert) {G_\sigma}_r(I_p - I_q)I_q
\end{equation}
where $C(I)_p$ is the output intensity of the filtered pixel p. ${G_\sigma}_s$
is a spatial gauss filter which decreases the influence of pixel q with
increasing distance between p and q.
${G_\sigma}_r$ is a range gauss filter, meaning it decreases the influence of
pixel q with increasing intensity difference between p and q.
Their intensities are denoted as $I_q$ and $I_p$ respectively.
S represents the set of pixels close to p and is determined by a diameter d
around each pixel.
The normalization factor $W_p$ is given by:
\begin{equation}
    W_p = \sum_{q \in S}^{} {G_\sigma}_s(\lVert p - q \rVert) {G_\sigma}_r(I_p - I_q)
\end{equation}
Compared to a simple low pass filter, which is as well implemented as gaussian
filter, the bilateral filter takes advantage of not only considering the
spatial relation between a pixel and its neighbors but also the relation
between their intensity values.
This allows a bilateral filter not only to smoothen high frequencies, but to
preserve sharp edges at the same time.
Opencv offers the built-in method \texttt{bilateralFilter()} to apply a
bilateral filter to an input video frame.
Besides the frame that should be filtered, it also takes $\sigma_s$ and
$\sigma_r$ as parameters, which define the algorithms' sensitivity to
spatial- and intensity changes as well as the diameter d in pixels.\\
As the filters time complexity depends on the region around each pixel that
is taken into account in the gaussian weighting process, it is implemented as
first approach to smoothen an input video frame without losing important
details in the regions relevant for the body key point detection.

\subsubsection*{Detecting body key points}\label{subsubsec:4_detecting_body_keypoints}
Each pre processed video frame is passed to the key point detector stage.
This stage is based on the mediapipe pose detection framework (see
\autoref{subsec:2_mediapipe_framework} and \autoref{subsec:2_why_mediapipe}).
The models used in this work are the BlazePose models.
BlazePose is a convolutional neural network with an architecture similar to
MobileNetV2.\\
It is capable of detecting a total of 33 body key points (shown in
\autoref{fig:2_body_keypoints}).
All of them are tried to be detected in each frame, while only few of them are
used in the following parameter calculation process.\\
The BlazePose network is able to work directly with RGB images as.
Thus, the filtered results form the pre-processing stage are as well in the
RGB color space.
The output generally is a 5-tuple for each detected key point.
Besides \texttt{x and y} coordinates, BlazePose also offers a
\texttt{depth estimation} and values describing the probability of a key point
being \texttt{present} and \texttt{visible} in the current frame.
The x and y values are normalized according to the image width and height.
Furthermore, BlazePose offers three different models\footnote{light, full and
heavy} which differ in the amount of parameters, thus in the networks'
complexity and the resulting detection accuracy.
Their inference times range from 0.02s to 0.25s per input video frame
respectively.
Hence, the least complex model can also be used for real time pose detection.\\
This project supports all three models, as a quick and less accurate analysis
can be sufficient (especially in good lighting and recording conditions),
whereas the best detection results, especially in poor quality videos are
achieved by using the heavy model.\\
The detected poses can be visualized as can be seen in following
\autoref{fig:4_pose_detection_in_out}: 
\begin{figure}[h!]
    \begin{subfigure}[b]{0.47\textwidth}
        \includegraphics*[scale=0.1]{takeoff_good.png}
        \caption{Sample input frame}
        \label{subfig:input_frame}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.5\textwidth}
        \includegraphics*[scale=0.185]{takeoff_keypoints.png}
        \caption{Pose detection output}
        \label{subfig:output_pose_detection_result}
    \end{subfigure}
    \caption[Pose detection example]{Sample pose detection output.\\
    (a) is the input video frame and (b) is the pose detector output.
    The input frame was pre processed using a high pass filter.}
    \label{fig:4_pose_detection_in_out}
\end{figure}

\subsubsection*{Arm- and knee angle calculation}
Based on the previously detected body key points, the parameters shown in
\autoref{fig:4_long_jump_sketch} can be calculated.
Especially knee angles during the run up- and takeoff phase can give important
insights into an athletes' overall jumping dynamics.
In this context, lower knee angles during the run up (at the toe-off point) as
well as during the takeoff (swing leg) were found to lead to better jumping
results\cite{mattes_kinematic_2021}.
Thus, these parameters are calculated by the software and can be saved and
visualized afterwards.\\
In the following, the angle calculation is explained based on the knee angle
calculation.
Taking the relevant detected key points into consideration, following
situation is given:
\begin{figure}[!h]
    \centering
    \includegraphics[scale=0.9]{knee_angles_calc.pdf}
    \caption[Knee angle calculation]{Knee angle calculation and related body
    key points.\\
    \textcolor{red}{Red dots} represent the body key points needed to calculate
    the knee angle. $\vec{hk}$ and $\vec{ka}$ denote the hip-knee vector and
    the knee-ankle vector respectively.}
    \label{fig:4_knee_angle_calculation}
\end{figure}
\FloatBarrier
\noindent The vectors $\vec{hk}$ and $\vec{ka}$ can be calculated using the
detected key points' coordinates:
\begin{equation}\label{eq:knee_angle_vectors}
    \vec{hk} = \begin{bmatrix}
        knee.x - hip.x \\
        knee.y - hip.y
    \end{bmatrix} \\
    \text{\ and \ }
    \hfill
    \vec{ka} = \begin{bmatrix}
        ankle.x - knee.x \\
        ankle.y - knee.y
    \end{bmatrix}
\end{equation}
\noindent Where .x and .y denote the key points' respective x and y coordinates.\\
The knee angle $\theta_{knee}$ is then represented by the angle
between $\vec{hk}$ and $\vec{ka}$ and can be calculated by using the scalar
product of $\vec{hk}$ and $\vec{ka}$ and their norms (lengths):
\begin{equation}\label{eq:knee_angle}
    \theta_{knee} = \arccos\left(\frac{\vec{hk} \cdot \vec{ka}}{\lvert \vec{hk} \rvert \cdot \lvert \vec{ka} \rvert}\right)
\end{equation}
where $\cdot$ defines the scalar product in this case.
The scalar product and the norm are calculated using numpys'
\texttt{dot()} and \texttt{norm()} methods.\\
The arm angle calculation is performed analogously and is therefore not shown
in detail.\\
As the knee- and arm angle calculations have to be done for each video frame,
they have to be implemented efficiently not to negatively influence the
overall analysis time.
Thus, numpy is used to perform the calculations.
\begin{pythoncode}[caption=Angle calculation,label=alg:angle_calc_algo]
    def calc_angle(first_vec, second_vec):
        first_vec_norm = np.linalg.norm(first_vec)
        second_vec_norm = np.linalg.norm(second_vec)
        if first_vec_norm == 0 or second_vec_norm == 0:
            return np.nan
        return 180 - np.rad2deg(
            np.arccos(
                (np.dot(first_vec, second_vec))
                / (first_vec_norm * second_vec_norm)
            )
        )
\end{pythoncode}
The above shown \texttt{calc\_angle()} function takes the two vectors as
arguments between which the angle should be calculated.
These vectors are, in case of the arm- and knee angles, directly given by the
detected body key point positions (see
\autoref{subsubsec:4_detecting_body_keypoints} and
\autoref{eq:knee_angle_vectors}).\\
%TODO%
By using this implementation, the overall angle calculation is sufficient fast
(performance analysis are shown in \autoref{subsec:4_runtime_performance})
in comparison to the keypoint detection process in order to allow for a quick
on-field analysis.

\subsubsection*{Takeoff angle calculation}
As can be seen in \autoref{fig:4_long_jump_sketch}, in the moment of the
takeoff, the \ac{CM}'s vertical velocity changes its direction rapidly.
This change can be quantised by the angle around which the overall
velocity vector rotates.
The calculated angle is in the following referred to as \textit{takeoff angle}
which is one of the most important pre-jump parameters.
As shown in~\cite{seyfarth_optimum_2000}, even small deviations of around
1\textdegree\ of the optimal takeoff angle can lead to shorter measured
distances (see \autoref{fig:4_long_jump_sketch}) of up to 5~cm.
The optimum takeoff angle differs between athletes and is especially
correlated to the takeoff speed~\cite{tsuboi_mathematical_2010-1}.\\
To calculate the takeoff angle, two vectors are needed.
The first vector is a horizontal vector origin in the athletes \ac{CM}.
The second vector represents the athletes'velocity, which, in this case, 
is considered equal to their \ac{CM}'s velocity.
This vector is calculated based on the takeoff frame, which needs to be
determined first (see \autoref{subsec:4_takeoff_detection}).
The second frame which is used to calculate the velocity vector is chosen
according to an offset f based on the videos' frame
rate\footnote{Chosen as $\frac{1}{10}$th of the frame rate}.
The \ac{CM}'s velocity vector is therefore given by:
\begin{equation}
    \vec{{}v}_t = \frac{1}{f} * (\vec{{}x}_{t+f} - \vec{{}x}_t)
\end{equation}
where f is the frame offset and $\vec{{}x}_i$ are the position vectors.
Here, $\vec{{}x}_t$ represents the \ac{CM} position at the moment of the takeoff
whereas $\vec{{}x}_{t+f}$ denotes the \ac{CM} position f frames later.\\
\autoref{eq:knee_angle} can then be applied again yielding following equation
for the takeoff angle:
\begin{equation}\label{eq:takeoff_angle}
    \theta_{\text{takeoff}} = \arccos\left(\frac{\vec{v_t} \cdot \vec{h_t}}{\lvert\vec{v_t}\rvert \cdot \lvert\vec{h_t}\rvert}\right)
\end{equation}
where $\vec{h_t}$ denotes any horizontal vector, e.g. $\vec{h_t} = [1, 0]$.\\
\autoref{eq:takeoff_angle} is calculated by using the function shown in
\autoref{alg:angle_calc_algo} by passing the two vectors shown in following
\autoref{fig:4_takeoff_angle_calculation}:
\begin{figure}[!h]
    \centering
    \begin{subfigure}{0.45\textwidth}
        \includegraphics[scale=0.1]{takeoff_angle_overlay.jpg}
        \caption{Poses shown as overlay}
    \end{subfigure}
    \begin{subfigure}{0.45\textwidth}
        \includegraphics[scale=0.1]{takeoff_angle_no_overlay.jpg}
        \caption{Extracted vectors and positions}
    \end{subfigure}
    \caption[Takeoff angle]{Takeoff frame annotated with the
    \textcolor{green}{two vectors} that define the takeoff angle marked
    green.\\}
    \label{fig:4_takeoff_angle_calculation}
\end{figure}
\FloatBarrier
\noindent The horizontal vector in above \autoref{fig:4_takeoff_angle_calculation}
represents $\vec{h_t}$ in \autoref{eq:takeoff_angle} and the other vector
visualizes $\vec{v_t}$ respectively.
Moreover, the velocity vector is scaled by a factor of 20 due to visualization
reasons.\\

\subsection{Automatic takeoff frame detection}\label{subsec:4_takeoff_detection}
Generally, a long jump can be divided into three different phases, namely
approach, jump and landing.
In \autoref{fig:4_long_jump_sketch}, an overview of the phases is presented,
with the takeoff phase denoted by the first phase transition point ($PT_0$).\\
Of all phases, understanding the dynamics of the takeoff phase is especially
important, as it represents the last opportunity for an athlete to actively
influence critical jumping conditions (e.g.\ takeoff speed, takeoff angle,
\dots).
Within this short\footnote{0.1s - 0.2s\cite{mechanical_power_long_jump}} time
period the initial kinetic run-up energy is mostly transformed into elastic
energy\cite{wittersModelElasticTake1992}.\\
Moreover, the velocity vector of an athletes' \ac{CM} changes its direction
in the moment of the takeoff\cite{murakiJointTorquePower2008}, as illustrated in
\autoref{fig:4_long_jump_sketch}.
The forces produced during the takeoff strongly influence the resulting
jumping distance\cite{hayCitiusAltiusLongius1993}.\\
Due to the short time period a takeoff takes, it can be difficult to detect
the takeoff frame in a long jump video in order to be able to analyze the
exact pre-jump conditions.\\\\
However, especially to allow for an on-field jumping analysis, a quick takeoff
frame detection is crucial.
Thus, an algorithm that can automatically detect the takeoff frame in a long
jump video is implemented in the following.\\\\
As the takeoff frame detection in this project is based on the parameters
listed in \autoref{sec:4_analysis_software}, the left and right
foot position as well as the position of the \ac{CM} were analyzed in
pre-recorded long jump videos of male long jumpers at different professional
levels reaching from hobby- to olympian athlete.
Moreover, the videos used differ in their length and quality as well as in the
jumping part they show (e.g.\ some videos do not show the full run-up).
The results of three exemplary video analysis are shown in
\autoref{fig:4_angles_height_plot}.
In order to develop an accurate takeoff point detection, the takeoff frame was
first manually selected (marked as \textcolor{blue}{blue vertical line} for
each analysis in \autoref{fig:4_angles_height_plot}).\\
The presented data has not been cleaned up in any way.
This can especially be seen in the first and last analysis, in which the
position and angle data is not accurate due to body key point detection
inaccuracies.
These inaccuracies however are not of great interest at this point as the key
points are detected correctly during the approach after a few frames.\\\\
In each analysis the left and right foot positions as well as the
according knee angles interchange from step to step.
The relative \ac{CM} height remains on the same level during the approach.\\
Behind the takeoff point, the relative hip- and foot heights increase quickly
until the maximal height is reached and the landing phase starts.
Moreover, the swing legs' foot height increases faster in comparison to the
jumping legs' foot height as the latter one stays on the ground longer to
introduce the jump.\\
In the moment of the takeoff, the knee angle of the jumping leg is above 170
degree, meaning the jumping leg is fully extended.
The swing legs' knee angle varies in the shown examples around 100 degree.\\\\
The visually most significant change which happens in the moment of the
takeoff and is therefore considered the most meaningful measured parameter
that indicates the takeoff is the \ac{CM}'s rapid height change.
Thus, the automatic takeoff detection is developed based on this parameter.\\
To approach the detection of the change in the \ac{CM}s' vertical
velocity, a mathematical description of its position during the jump is
modelled.

\begin{figure}[h!]
    \begin{subfigure}[b]{0.5\textwidth}
        \includegraphics*[scale=0.45]{jump_runup_poor_start.png}
        \captionsetup{justification=centering, singlelinecheck=false, labelfont=bf}
        \label{subfig:runup_jump_landing_height}
    \end{subfigure}
    \begin{subfigure}[b]{0.5\textwidth}
        \includegraphics*[scale=0.45]{jump_runup_poor_start_angles.png}
        \label{subfig:runup_jump_landing_angles}
    \end{subfigure}
    \begin{subfigure}[b]{0.5\textwidth}
        \includegraphics*[scale=0.45]{jump_no_runup.png}
        \label{subfig:no_runup_height}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.5\textwidth}
        \includegraphics*[scale=0.45]{jump_no_runup_angles.png}
        \label{subfig:no_runup_angles}
    \end{subfigure}
    \begin{subfigure}[b]{0.5\textwidth}
        \includegraphics*[scale=0.45]{jump_runup_poor.png}
        \label{subfig:runup_jump_height}
    \end{subfigure}
    \begin{subfigure}[b]{0.5\textwidth}
        \includegraphics*[scale=0.45]{jump_runup_poor_angles.png}
        \label{subfig:runup_jump_angles}
    \end{subfigure}
    \caption[Analyzed jumping parameters over time]{Analyzed and calculated
    jumping parameters over time.\\
    \textbf{Left column}: Analyzed left / right foot height and hip height.\\
    \textbf{Right column}: Calculated knee angles over time.\\
    The \textcolor{blue}{vertical lines} represent the visually selected
    takeoff point.\\
    \textbf{First row}: full run-up, full jump.\ \textbf{Second Row}: short 
    run-up, full jump.
    \textbf{Third row}: full run-up, full jump, poor video quality}
    \label{fig:4_angles_height_plot}
\end{figure}
\FloatBarrier

\subsubsection{Regression}
As the takeoff point detection is based on linear and quadratic regression,
both are briefly introduced in the following.
Regression generally describes the approximation of a polynomial function to
fit a given dataset.
The dataset that is tried to be approximated in this case is the height of the
\ac{CM}.
The dataset can be expressed as (T, H), where H holds the heights of the
\ac{CM} and T holds the related time points.
As the input is a video, T represents a simple vector holding the
video's frame numbers.
For the following steps it is helpful to express T and H as column vectors:
\begin{equation}
    \vec{t} = \begin{bmatrix}
        1\\
        2\\
        \vdots\\
        n
    \end{bmatrix}
        \quad\text{and}\quad
    \vec{h} = \begin{bmatrix}
        h_1\\
        h_2\\
        \vdots\\
        h_n
    \end{bmatrix}
\end{equation}
where n is the total number of video frames and $h_i$ is the $i\--th$ recorded
height of the \ac{CM}.\\
Taking the \ac{CM} in \autoref{fig:4_angles_height_plot} into consideration,
a whole jump can hardly be fitted with a single polynomial function.
However, as shown in \autoref{fig:4_long_jump_sketch}, a long jump consists of
multiple phases.\\
This offers an opportunity to detect the takeoff.
As mentioned before, the takeoff frame is defined as the point where the
approach phase ends and the jumping phase starts, ($PT_0$ in
\autoref{fig:4_long_jump_sketch}).
Thus, if a mathematical expression can be found for each phase respectively,
the takeoff frame is found implicitly.
The algorithm used for this task is based on the algorithm for a takeoff
frame detection developed by Muniz \cite{muniz_detection_2019}.\\
Other than supposed in his work, the detection algorithm in this work is not
based on the foot positions, but on the \ac{CM} position.
There are several reasons for preferring the \ac{CM} over the foot positions.
One of the most important reasons lies in the different jumping techniques,
more specifically in the resulting differences during the jumping phase.
While the foot path during the jumping phase is rather smooth for
athletes using the \textit{hang technique}, it is very different for
athletes using the \textit{hitch-kick technique}.
Latter one is characterized by quick foot position interchanges, making it
more error-prone for inaccuracies in the corresponding body key point
detection.
Thus, approximating the jumping phase based on the foot positions is more
complex and less precise.
The \ac{CM} however follows a similar path independent from the jumping
technique, making it more suitable for the given purpose of detecting the
takeoff frame.\\\\
\noindent In the following, the algorithm is explained in detail.
All mentioned frame numbers and jumping phases refer to
\autoref{fig:4_long_jump_sketch}.\\
The runup phase includes all frames in the interval $[0, PT_0[$, the jumping
phase is covered by the frames in range $[PT_0, PT_1[$ and the landing phase
is shown by the frames in the interval $[PT_1, n[$, where $PT_0$ defines the
first Phase Transition point (the takeoff) and $PT_1$ the point at which the
landing phase starts.
Each phase by itself can be approximated by a polynomial function.
The approach and the landing can be fitted using a linear expression.\\
Thus, two linear regressions are performed separately to find two expressions
of the form
\begin{equation}\label{eq:linear_function}
    \\y = \beta_0 + \beta_1t
\end{equation}
which directly yields following linear regression function:
\begin{equation}\label{eq:linear_reg_function}
    \\y_i = \beta_0 + \beta_1t_i + \epsilon_i
    \quad\quad
    i = \text{start},\ \dots\ ,\text{end}
\end{equation}
where $\beta_0$ and $\beta_1$ are the coefficients that need to be found.
start and end represent the first and last frame index that should be
considered in the linear regression.
For the approach phase this leads to $start = 0$ and $end = TP_0 - 1$, the
landing phase is accordingly represented by $start = TP_1$ and $end = n - 1$.\\
\autoref{eq:linear_reg_function} can also be expressed as matrix equation:
\begin{equation}\label{eq:linear_reg_matrix}
    \underbrace{
    \begin{bmatrix}
        y_0\\
        y_1\\
        \vdots\\
        y_r\\
    \end{bmatrix}}_{\substack{\vec{y}}}
    =
    \underbrace{
    \begin{bmatrix}
        1 & t_{\text{start}}\\
        1 & t_{\text{start} + 1}\\
        \vdots & \vdots\\
        1 & t_{\text{end}}
    \end{bmatrix}}_{\substack{\text{Design matrix} \\ T}}
    \underbrace{
    \begin{bmatrix}
        \beta_0\\
        \beta_1
    \end{bmatrix}}_{\substack{\text{Coefficients} \\ \text{vector} \\
    \vec{\beta}}}
    +
    \underbrace{
    \begin{bmatrix}
        \epsilon_0\\
        \epsilon_1
    \end{bmatrix}}_{\substack{\text{Error} \\ \text{vector} \\
    \vec{\epsilon}}}
\end{equation}
where $r = end - start$.
This can be written in short form as:
\[
    \vec{y} = T\vec{\beta} + \vec{\epsilon}    
\]
Now, the vector $\vec{\beta}$ needs to be found that minimizes the sum of
errors $E_{SSE}$ between the measured \ac{CM} heights $h_i \in \vec{h}$ and
the fitted polynomial $y_i$ at time step i.
To measure the error, the \ac{SSE} is used:
\begin{equation}
    E_{SSE} = \sum_{i=start}^{end} \epsilon_i^2 = \sum_{i=start}^{end} (h_i - y_i)^2
\end{equation}
Using the simple linear expression from \autoref{eq:linear_reg_function},
following error function is found:
\begin{equation}
    E_{SSE} = \sum_{i=start}^{end} (h_i - \beta_0 - \beta_1i)^2
\end{equation}
By using the matrix notation introduced in \autoref{eq:linear_reg_matrix},
the equation above can be written in matrix notation as well:
\begin{equation}\label{eq:sse_matrix}
    E_{SSE} = (T\vec{\beta} - \vec{h})^T(T\vec{\beta} - \vec{h})
\end{equation}
As $\vec{\beta}$ should minimize $E_{SSE}$, the minimum of $E_{SSE}$ is
needed, which can be found by solving following equation:
\begin{equation}\label{eq:gradient_linear_case}
    \nabla_{\vec{\beta}}E_{SSE} = \nabla_{\vec{\beta}}(T\vec{\beta} - \vec{h})^T(T\vec{\beta} - \vec{h}) = 0
\end{equation}
where $\nabla_{\vec{\beta}}$ denotes the gradient with respect to $\vec{\beta}$.\\
The coefficients can then be found by solving
\autoref{eq:gradient_linear_case} for $\vec{\beta}$, which yields the 
following normal equation:
\begin{equation}\label{eq:normal_equation}
    \vec{\beta} = (T^T T)^{-1}(T^T\vec{h})
\end{equation}
that especially requires $(T^{T} T)$ to be invertible.
A proof of \autoref{eq:normal_equation} can be found in
\cite{proof_linear_regression_mat}.\\
As mentioned, this process is performed twice to find a linear approximation
for the approach and the landing phase of an athletes' \ac{CM} respectively.\\\\
The jumping phase cannot be approximated with a linear polynomial.
However, as can be seen in \autoref{fig:4_angles_height_plot} after the marked
takeoff point, the curve of the \ac{CM} height can be approximated using a
parabola.
Thus, a quadratic regression is used to find a curve that describes the height
of the \ac{CM} during the jumping phase.
The second order polynomial that needs to be found is of the form:
\begin{equation}\label{eq:quadratic_function}
    \\y = \beta_0 + \beta_1t + \beta_2t^2
\end{equation}
yielding the quadratic regression function:
\begin{equation}\label{eq:quadratic_reg_function}
    \\y_i = \beta_0 + \beta_1t_i + \beta_2t_i^2 + \epsilon_i
    \quad\quad
    i = PT_0,\ \dots\ ,PT_1 - 1
\end{equation}
$PT_0\ \text{and}\ PT_1$ are the phase transition points shown in
\autoref{fig:4_long_jump_sketch}.
The following steps are equal to the linear regression shown above.
Thus, only the differences are shown in detail.\\
The T matrix in \autoref{eq:linear_reg_matrix} contains all frame
numbers as column vector.
Because a linear relation was tried to be found to approximate the approach
and  landing phase before, the T matrix as well only contained linear values.
Now however, a quadratic relation needs to be found.
Thus, the T matrix needs to be extended by one more column holding the
quadratic frame numbers yielding following matrix equation:
\begin{equation}\label{eq:quadratic_reg_matrix}
    \underbrace{
    \begin{bmatrix}
        y_0\\
        y_1\\
        \vdots\\
        y_r\\
    \end{bmatrix}}_{\vec{y}}
    =
    \underbrace{
    \begin{bmatrix}
        1 & t_{PT_0} & t_{PT_0}^2\\
        1 & t_{PT_0 + 1} & t_{PT_0 + 1}^2\\
        \vdots & \vdots & \vdots\\
        1 & t_{PT_1 - 1} & t_{PT_1 - 1}^2
    \end{bmatrix}}_{T}
    \underbrace{
    \begin{bmatrix}
        \beta_0\\
        \beta_1\\
        \beta_2
    \end{bmatrix}}_{\vec{\beta}}
    +
    \underbrace{
    \begin{bmatrix}
        \epsilon_0\\
        \epsilon_1\\
        \epsilon_2
    \end{bmatrix}}_{\vec{\epsilon}}
\end{equation}
where $r = PT_1 - 1 - PT_0$.
The error function which needs to be minimized can be set equivalent to the
linear case (\autoref{eq:sse_matrix}).
Comparing \autoref{eq:linear_reg_matrix} and \autoref{eq:quadratic_reg_matrix}
the only things that change are the number of coefficients (and thus the
number of error terms) as well as the design matrix T, which holds one more
column.
The steps to determine the coefficients $\vec{\beta}$ are equal to the linear
case in equations~\ref{eq:sse_matrix} to~\ref{eq:normal_equation}.
$\vec{\beta}$ is then given by the same normal equation~
\ref{eq:normal_equation} as in the linear case:
\[
    \vec{\beta} = (T^T T)^{-1}(T^T\vec{h}) 
\] 
where $\vec{\beta}$ contains three coefficients $\beta_0$, $\beta_1$ and
$\beta_2$ to approximate the jumping phase.\\\\

\noindent By using this linear- and quadratic regression approach, the whole
jump can be modelled mathematically.
However, in order to find the best fitting model, the phase changing points
($PT_0$ and $PT_1$ in \autoref{fig:4_long_jump_sketch}) need to be determined.
This can be done by minimizing the overall error $E_{total}$, which is defined
as the sum of regression errors resulting from the three independent
regressions performed (one per jumping phase):

\begin{equation}\label{eq:total_error}
    E_{total} = E_{approach} + E_{jump} + E_{landing}
\end{equation}

\noindent $E_{total}$ can be minimized by using a brute-force approach.
For each possible combination of phase transition points, two linear
regressions (representing approach and landing) and one quadratic regression
(representing the jumping phase) are performed and their individual errors
given by \autoref{eq:sse_matrix} are summed up.
The combination of phase transition points that leads to the minimal
$E_{total}$ is then considered as the optimal model to fit the overall jump.
As the found phase transition points directly represent their corresponding
frame numbers, the takeoff frame is directly given by the first phase
transition point.\\
The described process is outlined in following
Algorithm~\ref{alg:takeoff_frame}.
\begin{algorithm}[h!]
    \caption{takeoff\_frame(cm\_height: array, knee\_angles: array)}
    \begin{algorithmic}[1]
        \State $n \gets \text{length of } cm\_height$
        \State $total\_error \gets \text{initial error}$
        \State $transition\_points \gets (0,0)$
        \For{$i \gets 0$ \textbf{to} $n - 1$}
            \For{$j \gets i + 1$ \textbf{to} $n$}
            \State $\vec{\beta_{approach}}, error\_approach \gets \text{lin\_reg}(hip\_height[:i])$
            \State $\vec{\beta_{jump}}, error\_jump \gets \text{quad\_reg}(hip\_height[i:j])$
            \State $\vec{\beta_{landing}}, error\_landing \gets \text{lin\_reg}(hip\_height[j:])$
            \State $fitting\_error \gets error\_approach + error\_jump + error\_landing$
                \If{fitting\_error $<$ total\_error \textbf{and} $\vec{\beta_{jump}}[0]$ $<$ 0 \textbf{and} knee\_angles[i] > 170}
                    \State $total\_error \gets$ fitting\_error
                    \State $transition\_points \gets (i,j)$
                \EndIf
            \EndFor
        \EndFor
        \State \textbf{return} transition\_points
    \end{algorithmic}
\label{alg:takeoff_frame}
\end{algorithm}
\FloatBarrier

\noindent The expression \texttt{$\vec{\beta_{jump}}[0] < 0$} guarantees, that
the found parabola for approximating the jumping phase is opened downwards.
Moreover, \texttt{$knee\_angles[i] > 170$} makes sure, that only frames in
which the knee angle is above 170 degrees can be considered as takeoff point.\\
The shown algorithms' runtime scales according to $\mathcal{O}(n^2)$
where n is the number of video frames~\footnote{in which body key points were
found}. 
Thus, it is not suitable for long video sequences.
However, as a complete long jump takes only a few seconds, the algorithm can
be used for an on-field jump analysis.\\
Algorithm~\ref{alg:takeoff_frame} was then applied to all three datasets shown
in \autoref{fig:4_angles_height_plot}. 
The results are shown in \autoref{fig:4_automatic_takeoff_results}.\\
For better understanding, the height of the \ac{CM} was extracted from the
datasets as it is the only parameter relevant for the takeoff detection.
The data is not cleaned up in any way, thus, outliers are still
visible.
However, as can be seen, the automatically detected takeoff frames are
similar to the manually annotated frame numbers.
\begin{figure}[h!]
    \centering
    \begin{subfigure}{0.45\textwidth}
        \includegraphics[width=\textwidth]{regression_jump_only.png}
        \caption{Short approach, reliable keypoint quality}
    \end{subfigure}\hfill
    \begin{subfigure}{0.45\textwidth}
        \includegraphics[width=\textwidth]{regression_poor_approach.png}
        \caption{Long approach, outliers in approach}
    \end{subfigure}
    \begin{subfigure}{0.45\textwidth}
        \includegraphics[width=\textwidth]{regression_poor_approach_jump.png}
        \caption{Long approach, outliers in approach and jump}
    \end{subfigure}
    \caption[Automatic takeoff frame detection results]{
        All three jumping phases approximated using linear regression
        (runup and landing) and quadratic regression (jumping phase).\\
        The automatically detected takeoff frame is marked with a
        \textcolor{red}{red dot}.\\
        The \textcolor{cyan}{vertical blue lines} represent the manually marked
        takeoff frames.}
\label{fig:4_automatic_takeoff_results}
\end{figure}
\FloatBarrier

\subsection{Saving analysis results}\label{subsec:4_param_files}
After a complete jump analysis has been performed, all data, especially
the calculated parameters such as arm- and knee angles need to be stored
permanently.
Thereby, each input video file must only be analyzed once in order to avoid
costly double analysis.
While the detected body key points are stored in form of an annotated video
file, the analysis parameters are stored alongside in a separate file which
can be re-loaded and visualized.
This file will in the following be referred to as \texttt{Parameter file}.\\
As most of the parameters are calculated frame-wise (like knee angles, foot
positions, \dots), the well-structured hdf5 file format (see
\autoref{subsec:2_hdf5}) is used for this task.
The exact file structure is shown in following 
\autoref{fig:hdf5_file_structure_param_file}:
\begin{figure}[h!]
    \centering
    \scalebox{0.8}{ % Adjust the scaling factor as needed
        \begin{minipage}{0.5\textwidth}
            \dirtree{%
            .1 Root.
            .2 \textcolor{cyan}{video\_file}.
            .2 \textcolor{cyan}{transition\_points}.
            .2 \textcolor{cyan}{takeoff\_vector}.
            .2 Frame0.
            .3 right\_foot\_x.
            .3 right\_foot\_y.
            .3 left\_foot\_x.
            .3 left\_foot\_y.
            .3 left\_knee\_angle.
            .3 right\_knee\_angle.
            .3 cm\_x.
            .3 cm\_y.
            .2 Frame1.
            .3 \dots.
            .2 Frame2.
            .3 \dots.
            .2 \dots.
        }
        \end{minipage}
    }
    \caption[HDF5 Parameter file structure]{HDF5 Parameter file structure}
    \label{fig:hdf5_file_structure_param_file}
\end{figure}
\FloatBarrier
\noindent As can be seen in above \autoref{fig:hdf5_file_structure_param_file},
each hdf5 file contains three \textcolor{cyan}{metadata} parameters.
They do not belong to a certain frame and represent important analysis results
that take several frames into consideration.
Furthermore, these parameters can be accessed efficiently.
The \texttt{transition\_points} metadata contains the phase transition points
(see \autoref{fig:4_long_jump_sketch}), whereas \texttt{takeoff\_vector} is a
tuple of the form ($\theta_{\text{takeoff}}, \vec{{}v}_t$) according to
\autoref{eq:takeoff_angle}.
As one of the shown Parameter files is created per jump analysis, an annotated
video file belongs to each of those files.
Thus, a connection to the related video file is saved in the
Parameter file in form of the \texttt{video\_file} metadata.\\
Furthermore, there is a group created for each analyzed video frame.
Within these groups there is one dataset created for each analysis parameter.\\

\noindent To allow simple and efficient read- and write operations, the class
\texttt{Parameterfile} is implemented, which abstracts the described parameter
file.
All HDF5 file interactions are implemented using the python
h5py package\cite*{collette_python_hdf5_2014}, which offers dedicated methods
for creating a whole HDF5 file as well as methods for creating groups,
datasets and metadata.\\
Taking into consideration that, for each input video frame, one group in the
according HDF5 file as well as one annotated video frame needs to be saved,
there are many memory accesses during the analysis process, which leads to
poor runtime behavior.
To reduce the number of memory accesses, the Parameterfile instances buffer
some HDF5 frame groups and then perform batch saving.
This effectively reduces the number of costly memory accesses, leading to
a better performance behavior.
The software uses a batch size of 128 by default.\\
Overall, the \texttt{Parameterfile} class manages all analysis file
interactions.
Possible interactions can be seen in its class diagram:
\begin{figure}[!h]
    \centering
    \includegraphics[scale=0.6]{Parameterfile.pdf}
    \caption[Parameterfile class diagram]{Parameterfile class diagram}
    \label{fig:4_param_file_class_diagram}
\end{figure}
\FloatBarrier
\noindent In above \autoref{fig:4_param_file_class_diagram} the
\texttt{load()} method and all \texttt{get\_x()} methods are used to retrieve
analysis parameters from saved Parameterfiles.


\subsection{Runtime considerations}\label{subsec:4_runtime_performance}
The software is mainly developed towards a high key point detection and
thus, a high parameter calculation accuracy.
However, as it is meant to be used as an on-field analysis tool, quick
analysis times are important.
Hence, in the following, some important run times are shown and discussed.\\
All following evaluations were performed on a laptop equipped with an
AMD Ryzen 5800U processor and 16GB of ram.
This setup was chosen to represent a standard setup that can be used for an
on-field analysis.\\
Generally, there are two main runtime critical parts in the jump analysis
process: first, the key point detection and second, the takeoff frame
detection.
The first one mainly depends on the users' choice of the underlying key point
detection model (evaluated later).
The latter one however performs many numerical operations.
As a result, it can potentially profit from pre-compiled implementations
using \texttt{numba} (see \autoref{subsec:3_precompile_numba}).\\
To validate this assumption some runtime comparisons based on video inputs of
10 seconds length are performed.
One is recorded at 60 \ac{FPS} and the other one at 90 \ac{FPS},
resulting in 600 and 900 frames respectively.
The results are presented in following
\autoref*{fig:takeoff_frame_detection_performance}:
\begin{figure}[!h]
    \begin{subfigure}[b]{0.48\textwidth}
        \includegraphics*[height=5cm]{absolute_runtime.png}
        \caption{Absolute runtime comparison.\\
        Error bars represent the standard deviation.}
        \label{subfig:absolute_runtime_tkf_frame}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.48\textwidth}
        \includegraphics*[height=5cm]{relative_throughput.png} 
        \caption{Normalized throughput comparison.\\
        Values are directly proportional to the runtime shown in (a).}
        \label{subfig:relative_throughput_tkf_frame}
    \end{subfigure}
    \vfill
    \begin{subfigure}[b]{0.48\textwidth}
        \includegraphics*[height=5cm]{absolute_runtime_900.png}
        \caption{Absolute runtime comparison.\\}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.48\textwidth}
        \includegraphics*[height=5cm]{relative_throughput_900.png} 
        \caption{Normalized throughput comparison.\\}
    \end{subfigure}
    \caption[Takeoff frame detection performance]{Runtime and throughput
    comparison between pre-compiled (\texttt{numba}) and non
    pre-compiled takeoff frame detection implementations.}
    \label{fig:takeoff_frame_detection_performance}
\end{figure}
\FloatBarrier
\noindent As can be seen, the takeoff frame detection generally scales
according to $\mathcal{O}(n^2)$, where n is the number of analyzed video
frames (explained in \autoref{subsec:4_takeoff_detection}).
Looking at a 900 frame video, this leads to $900 * 900 = 810*10^3$ loop runs.
Moreover, in each iteration, three polynomial regressions are performed which
leads to around $2.4*10^6$ regression operations.\\
Due to that many regression operations, the two evaluated implementations
differ in the way they perform the necessary regressions (see Algorithm
\ref{alg:angle_calc_algo} for details).
While the python implementation relies on \texttt{numpy}'s
polyfit\footnote{\url{https://numpy.org/doc/stable/reference/generated/numpy.polyfit.html}}
method, the pre-compiled \texttt{numba} implementation uses a custom
regression implementation which is set up according to equations
\ref{eq:linear_reg_matrix} to \ref{eq:normal_equation}.
A custom implementation is chosen, as the polyfit method offered by
\texttt{numpy} cannot be pre-compiled using \texttt{numba}.
The complete implementation is not shown, however, the main function is
presented in order to demonstrate the (simple) usage of \texttt{numba}:\\
\begin{pythoncode}[caption=Polynomial regression,label=alg:poly_fit_numba]
    import numba as nb

    @nb.jit('f4[:](f4[:], f4[:], i4)') # numba decorator
    def _fit_poly(x: np.ndarray, y: np.ndarray, deg: int):
        A = _coeff_mat(x, deg) # creates design matrix
        p = _fit_x(A, y) # solves Ax = y
        return p[::-1] # ensure p[0] to be the highest order coefficient
\end{pythoncode}
The methods \texttt{\_coeff\_mat} and \texttt{\_fit\_x} represent
\autoref{eq:linear_reg_matrix} and \autoref{eq:normal_equation} respectively.
\texttt{\_fit\_x} uses the least-squares fitting approach shown in
\autoref{eq:gradient_linear_case} to find the best fitting model.
Furthermore, the residuals (errors) which are needed (see 
\autoref{eq:total_error}) are calculated via polynomial evaluation using
Horner's method~\cite{fuentesFastEvaluationPolynomials2022}.\\
As can be seen, the only difference to a standard python function definition
is the \texttt{nb.jit() decorator} which wraps the \texttt{\_fit\_poly}
function in \texttt{numba}s' jit function to pre-compile it when the function
is first called (therefore the name \textit{Just-in-time compiler}).\\
This behavior can potentially lead to a large execution overhead before the
function can be executed for the first time.
However, numba can also pre-compile the function when the application is
launched to avoid long compiling processes during runtime.
This is achieved by passing the parameter types to numba
(\texttt{'f4[:](f4[:], f4[:], i4)'}), where f4 denotes a standard 32 bit
single precision float, i4 a standard 32 bit integer value and [:] their array
types respectively.
Moreover, the first \texttt{f4[:]} defines the functions' return type
as an array of 32 bit floats.\\
Numba needs that extra information to pre-compile the function, as the
functions' parameter types cannot be derived just by the function definition,
as python generally is a dynamically typed language.\\\\
\noindent Taking the implementations and
\autoref{fig:takeoff_frame_detection_performance} into account, the takeoff
frame detection significantly profits from the pre -compiled numba
implementation.
The absolute runtime is reduced by over 80\%, increasing the throughout by a
factor of around 5.5.
Thus, the final takeoff frame detection is implemented using \texttt{numba}s'
JIT compiler.\\\\

\noindent The other runtime crucial part is the body keypoint detection.
As aforementioned, mediapipe offers three different BlazePose keypoint
detection models differing in terms of accuracy and performance.
All three models are implemented, thus their inference times are compared.
As the final analysis will be performed on videos of size 1920 x 1080, the
performance is tested on video frames of this size.
The results are presented in following:
\begin{figure}[!h]
    \begin{subfigure}[b]{0.48\textwidth}
        \includegraphics*[height=5cm]{inference_time_models.png}
        \caption{Absolute inference time comparison.\\
        Error bars represent the standard deviation.}
        \label{subfig:absolute_inference_times}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.48\textwidth}
        \includegraphics*[height=5cm]{inference_fps_models.png} 
        \caption{Theoretically achievable \acs*{FPS} comparison.\\
        Values are directly proportional to the runtime shown in (a).}
        \label{subfig:relative_inference_fps}
    \end{subfigure}
    \caption[Inference time comparison]{Inference time comparison of all three
    mediapipe pose detection models.}
    \label{fig:mp_model_inference_time_comparison}
\end{figure}
\FloatBarrier
\noindent As can be seen in above \autoref{fig:mp_model_inference_time_comparison}
the inference times significantly differ between the three BlazePose pose
detection models.
While the full, thus most complex and most accurate detection model achieves
speeds of up to around 10~\ac{FPS}, the light model can evaluate up to
40~\ac{FPS}.
This means latter one can theoretically even be used as a real time analysis
option for video recordings at 30~\ac{FPS}.
However, as this work focuses on the video analysis, the real time
capabilities are not part of this works' scope.\\
The parameter calculation (see \autoref*{sec:4_analysis_software}) is
neglected in the runtime measurements, as they are much less computational
expensive compared to the body key point detection and the takeoff frame
detection.\\
Thus, a full long jump video of 10~sec lengths (60~\ac{FPS}) can be fully
analyzed in around a minute.

\subsection{Visualization in a \acs*{GUI}}\label{subsec:4_lj_software_gui}
To present the jump analysis, a \ac{GUI} is implemented.
This \ac{GUI} is mainly developed regarding three purposes:
\begin{enumerate}
    \item Choose video files and analysis parameters.
    \item Present analysis results while the analysis is running.
    \item Present the fully analyzed video and the calculated parameters. 
\end{enumerate}
\noindent In the following, the \ac{GUI} will be shown and explained in two
steps.
At first, the first two points in above listing will be shown
and secondly, the full analysis visualization is presented.\\\\
\noindent The second point allows an athlete (or trainer) to analyze \textit{
single frame parameters} while the video is still being evaluated.
Single frame parameters are those, that do not need all frames to be evaluated
but just rely on the current frame (e.g. knee angles and arm angles).
In contrast a full analysis includes all other parameters, such as the takeoff
frame and the takeoff angle.
This allows to reduce the actual on-field analysis times, as a first overall
jumping impression can be offered immediately.\\
The \ac{GUI} that is used for the first and second point (see above) is
presented below:
\begin{figure}[h!]
    \centering
    \includegraphics[scale=0.9]{GUI_running_analysis.pdf}
    \caption[Analysis \acs*{GUI}]{\acs*{GUI} shown while the video analysis
    is running.}
    \label{fig:4_gui_analysis_running}
\end{figure}
\FloatBarrier
\noindent As can be seen, the \ac{GUI} is split into three main parts.
These are marked red, green and blue respectively in above
\autoref{fig:4_gui_analysis_running}.\\
The blue area is used to let the user choose important analysis parameters and
show the analysis progress.
The parameters are the filter function to be used (see
\autoref{subsubsec:4_filter_processing}) as well as the key point detection
accuracy (see \autoref{subsec:2_mediapipe_framework}) and the output mode.
Moreover, the athlete can choose to get the velocity vector calculated and
visualized in each frame.\\\\
\noindent Above this \textit{analysis control panel}, there is the actual
video widget, which is used to display the current video according to its
current analysis progress. All analysis results, that the user chose (in the
analysis control panel) are already visualized in this stage.
Particularly, the detected position overlay (the body key points) are shown.  
Thus, a first evaluation can be performed while the analysis is still running.\\\\
\noindent On the right hand side (red box), the actual analysis results are
visualized. In this stage, only single frame parameters are calculated and
shown. More specifically, these are knee angles, left- / right foot positions
as well as the hip height.
By updating these curves successively for each analyzed frame, an overview of 
the overall jumping progression is visualized over time.\\\\
\noindent The \ac{GUI} changes its layout slightly once the analysis is
finished to offer a more convenient way to interact with the analysis results
and their respective Parameter files (see \autoref{subsec:4_param_files}).
The \ac{GUI} layout for finished video analysis is presented in following
figure: 
\begin{figure}[h!]
    \centering
    \includegraphics[scale=0.7]{GUI_finished_analysis.pdf}
    \caption[Analysis results \acs*{GUI}]{\acs*{GUI} shown once the video
    analysis is finished or a completed analysis and its corresponding
    parameter file is loaded.}
    \label{fig:4_gui_analysis_finished}
\end{figure}
\FloatBarrier


\section{Drone setup}\label{sec:4_hardware}
In order to capture high-quality video recordings that cover a complete long 
jump, from the first step all the way to the landing, a drone is used to fly
next to the athlete throughout the whole process.
Thus, a drone in form of a quadcopter is built from scratch.
Its control will be integrated seamlessly in the projects' \ac{GUI}.\\
This section introduces the hardware components that are used for building 
this drone as well as its flight control unit.\\
A short outline of the hardware is given in 
\autoref{subsec:4_hardware_selection},
while \autoref{subsec:4_hw_setup} focuses on the overall assembly of the 
selected hardware.

\subsection{Hardware selection}\label{subsec:4_hardware_selection}
Currently, commercial drone hardware on the market is mainly separable into 
the two large areas of fully remote controlled \ac{FPV} hardware and hardware 
for (autonomous) drones that can usually carry more load, e.g.~heavy cameras.
Even though the quadcopter in this project needs to be remotely 
controllable from a ground station pc, it is still more likely to be located 
in the latter one.\\
Generally the hardware was chosen based on the following criteria:
\begin{itemize}
    \item price
    \item compatibility
    \item size
\end{itemize}

\subsubsection{Flight Hardware}\label{subsec:4_filght_hardware}
The main hardware that a quadcopter needs to fly will, in the following, be
referred to as \textit{flight hardware}.
This includes frame, motors, rotors, \acp{ESC} and a \ac{PDB}.\\
The main platform on which all drone hardware is mounted, is referred to as
a quadcopter's frame.
As this project's drone does not need to carry any heavy load, such as high 
precision camera systems or other sensors, a rather compact frame would 
theoretically be sufficient.
However, compact frames tend to be less stable compared to larger frame sizes 
which could lead to a lower video recording quality and thus require more 
complex post-processing software.
Moreover, the assembly process on larger frames is more convenient and 
replacing parts is easier.
Additionally, compact frames are most commonly used in areas that demand quick
reaction times for high speed flight maneuvers, e.g.~in drone racing.
This however is not needed in this project's context.\\
Taken the mentioned considerations into account the mid-sizes \textit{Holybro 
S500 V2} frame kit is chosen.
Besides the frame, the kit also includes a landing gear and rotors.
Moreover, the main platform includes a \ac{PDB} to split the battery's power 
equally to all four motors.\\
An overview of all included parts is given in \autoref{fig:4_frame_kit}.
\begin{figure}[!h]
    \centering
    \includegraphics[scale=0.6]{frame-kit.png}
    \caption[Frame kit]{Holybro S500 V2 frame kit}
    \label{fig:4_frame_kit}
\end{figure}
\FloatBarrier
\noindent Besides the frame, motors and compatible \acp{ESC} are crucial 
flight hardware components.
Each motor requires an own \ac{ESC} that translates signals from a flight 
control unit to a voltage and thereby control the motors' rotation speed.
To guarantee compatibility, both components were chosen from Holybro as well
and can be seen in \autoref{fig:motors_and_esc}.
\begin{figure}[!h]
    \begin{subfigure}[b]{0.48\textwidth}
        \includegraphics*[scale=0.15]{motor.jpg}
        \caption{920KV Motor}
        \label{subfig:motor_picture}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.5\textwidth}
        \includegraphics*[scale=0.15]{esc.jpg}
        \caption{\acl*{ESC}}
        \label{subfig:esc_picture}
    \end{subfigure}
    \caption[Motor and \acs*{ESC}]{Motor (a) and \acs*{ESC} (b)}
    \label{fig:motors_and_esc}
\end{figure}
\FloatBarrier
The drones' motors performance capabilities are defined by the number of 
\ac{RPM} they can perform per 1V input.
As can be seen in \autoref{subfig:motor_picture}, this link between 
rotation speed and input voltage is expressed in the arbitrary unit \textit{KV}.
The chosen motors are capable of rotating with a speed of 920~\ac{RPM} per 1V 
input voltage. 
Put into context, this is a common rotation speed in commercial and hobby 
drone applications.
Racing drones however, operate at motor speeds of up to 3500~KV.

\subsubsection{Control Hardware}\label{subsec:4_control_hardware}
In order to perform flight maneuvers with a quadcopter, each motor must be
controllable individually.
The calculation of the correct rotation speeds is generally performed by 
a \textit{flight control unit}.
Usually, it receives directional instructions from a remote control as input,
combines them with many parameters (e.g.~\acs{GPS} position, height over ground,
speed, etc.) and generates a (\acs{PWM}) output signal for each motor.\\
Within this project the flight controller needs to deal with two different 
inputs.
First, the ground station which can be seen as a remote control in this case.
Additionally, the drone should be able to fly autonomously next to an athlete
during their long jump training.
Here, the second input gets important.
The autonomous fly option requires the quadcopter to perform a person 
detection and therefore image processing on-board.
As the flight controller itself is not able to perform such calculations, an
additional \textit{companion computer} is required.
This companion computer will then send directional instructions just like the 
ones from the ground station to the flight controller and thereby control the 
drone.\\
The combination of flight controller and on-board companion computer will in 
the following be called \textit{control hardware}.\\
There are many types of different flight controllers available commercially.
However, most of them are not meant to be used in combination with a companion
computer.\\
Two of the most commonly used flight controllers in autonomous drone projects
are the \textit{PixHawk} and the \textit{Navio2}.
They are often chosen because they both work together seamlessly with a 
companion computer.
The former is a totally independent system which can also operate without any 
supporting computer.
The latter is implemented as \ac{HAT} specifically designed for a Raspberry 
Pi.
Thus, it does not include an own \ac{CPU} but uses the Raspberry Pis's 
resources to perform flight relevant calculations.\\
A detailed comparison between both flight controllers is given in 
\autoref{table:4_flight_cotroller_cmp}.
\begin{table}[h!]
    \centering
    \begin{tabular}[c]{|p{4cm}||p{4cm}|p{4cm}|}
    \hline
    \multicolumn{3}{|c|}{Flight Controller Comparison}\\
    \hline
    Criteria & PixHawk & Navio2\\
    \hline
    \hline
    Processor & ARM Cortex M4 with FPU / 32-bit co-processor & Depends on Raspberry
    Pi version\\
    \hline
    Sensors & ST Micro 16-bti gyroscope, ST Micro 14-bit accelerometer, MEAS
    barometer & MPU9250 9DOF IMU, LSM9DS1 9DOF IMU, MS5611 Barometer, U-blox M8N 
    Glonass/\acs{GPS}/Beidou\\
    \hline
    Interfaces & UART, Spektrum DSM, PPM / S.BUS input, I2C, SPI, CAN, USB, 3.3V 
    and 6.6V ADC input, 8 \acs{PWM} outputs, 6 Auxiliary outputs & UART, I2C, ADC, PPM / 
    S.BUS input, 14 \acs{PWM} outputs\\
    \hline
    Dimensions\newline(W x H x L) in mm & $50 \times 15.5 \times 81.5$ & $55 \times 65$\\ 
    \hline
    Other & Failsafe options (e.g. extra power supply, \acs{GPS}, etc.) & None\\
    \hline
    Price (Eur.) & TBD & TBD\\
    \hline
    \end{tabular}
    \caption[Flight controller comparison]{Comparison between PixHawk and 
    Navio2 flight controller.}
    \label{table:4_flight_cotroller_cmp}
\end{table}

\noindent As can be seen, both flight controllers offer different interfaces 
to connect additional hardware.
Moreover, both systems include sensors, mostly to gather information about 
drone's current position and inertia.
Here, the Navio2 even offers more sensors, as it already includes a \ac{GPS} 
sensor, while the PixHawk relies on an external one.\\\\
\noindent For this project, the PixHawk was chosen over the Navio2 mainly for 
three reasons.
First, it is on the market for a long time already and thus have a large 
community support.
Secondly, as it is an independent system, a failure of the companion computer 
will not lead to a crash.
Lastly, it allows for a wide range of companion computers, while the Navio2 
can only interoperate with a Raspberry Pi.\\
Furthermore, the mentioned considerations lead to easy rapid prototyping 
approaches, as the drone can be manually flown without a companion computer in
a first implementation.

\subsection{Hardware assembly}\label{subsec:4_hw_setup}
In the following, a high-level overview of the quadcopters' hardware setup is 
given.
The general wiring is shown and explained before a short introduction of some 
important communication protocols is given.

\subsubsection{General wiring}
In the following \autoref{fig:4_general_wiring} the general wiring layout is 
shown.
The whole system is powered from one power source only.
\begin{figure}[!h]
    \centering
    \includegraphics[scale=0.5]{general_wiring.pdf}
    \caption[General Wiring]{General wiring of the hardware setup.\\
    Power connections are labeled red, 
    Blue connections are used for \ac{PWM} signals, 
    Green connections are serial connections, 
    orange connections are more specifically serial UART connections.}
    \label{fig:4_general_wiring}
\end{figure}
\FloatBarrier
\noindent This results in some challenges in providing the correct voltage for
each connected device.
In the current setup this task is taken over by three devices.
The \ac{PM} is directly connected to the battery.
It transfers the battery's voltage to the \ac{PDB} and a lower 5V voltage to
the PixHawk flight controller.
The \ac{PDB} itself is a parallel circuit, thus providing the same voltage
(battery voltage) to each output.
The third device is a \ac{BEC} which is directly connected
to the \ac{PDB} and delivers a constant 5V output.
This can be used to power a companion computer such as a RaspberryPi.\\
All other required peripherals are powered by the PixHawk flight controller
itself.
The main peripherals used in this project are a telemetry module which is used
for communication with a ground station and a \ac{GPS} module used for
improving the drones capabilities to follow a defined trajectory, which is
specifically useful for auto-return~and landing features.
Two more peripherals, a buzzer to output audio warning signals and a manual
kill switch which can immediately stop all four motors, are installed mainly
for safety reasons.\\
The hardware components that actually control the motor rotation speeds,
the \acp{ESC}, are connected to the \ac{PDB} for power supply as well as to
the flight controller that calculates the correct rotation speeds based on the
wanted flight maneuvers and outputs a \ac{PWM} signal for each motor.\\
The presented overall wiring is rather complex but allows relying on one power
source only instead of using multiple power sources for flight hardware and
control hardware including peripherals respectively.

\subsubsection{Communication between components}\label{subsec:4_comm}
The drone setup needs hardware components to communicate with each other in 
order to transfer control signals from either the companion computer or from
the ground station to the flight controller.
Even if both options origin from different sources, they use the same
device-to-device communication protocol.
The protocol used for this purpose is the \textit{UART} protocol, which is
acronym for universal asynchronous receiver / transmitter protocol.
It is based on a serial, full duplex connection using six connections.
The connection layout is shown in \autoref{fig:4_uart_wiring},
\begin{figure}[!h]
    \centering
    \includegraphics[scale=0.8]{uart_wiring.pdf}
    \caption[UART wiring]{UART wiring}
    \label{fig:4_uart_wiring}
\end{figure}
\FloatBarrier
\noindent where RTS / CTS denote Ready to send and Clear to send.
RX / TX represent the actual data receiving and sending connections
respectively.\\

\noindent UART transfers data using data frames with minimal overhead.
A typical UART frame consists of just a start bit, data bits, a parity bit and
a stop bit.
\autoref{fig:4_uart_frame} visualizes such a data frame. 
\begin{figure}[!h]
    \centering
    \includegraphics[scale=0.8]{uart_frame.pdf}
    \caption[UART data frame]{Exmple of an UART data frame. The blue marked
    area is the actual data part that is transmitted.}
    \label{fig:4_uart_frame}
\end{figure}
\FloatBarrier
\noindent The shown UART data frame includes 5 data bits, however, the amount
can vary between 5 and 9.
Moreover, the included even parity bit is optional.
The idle state is set to a voltage that represents HIGH level on purpose, so
that any connection failure is easily detectable.
UART can work with any voltages to denote HIGH and LOW levels.
In the quadcopter setup HIGH is represented by 5V and LOW by GND.

\subsection{Recording and streaming videos}\label{subsec:4_rec_stream_video}
As the drone is meant to be used to support the long jump analysis process by
recording the jump and send the captured video to a ground station, a complete
video recording process is developed and implemented.\\
This includes the hardware camera setup on the drone, as well as the
communication between the ground station which initializes the video
recording and the drone capturing the actual video.\\
Generally, the RaspberryPi takes care of capturing the video recordings.
It starts the video recoridng when it is triggered via the ground station and
sends the recorded video file back to the ground station where it is analyzed.
Moreover, it offers a live stream of the current camera image to allow for
aligning the drone correctly relative to the athlete.\\
In the following, the implementations of the aforementioned features are shown
in detail.

\subsubsection{UDP sockets for communication}
\subsubsection{Camera setup and video recording}
\subsubsection{Live streaming the video feed}

\section{Controlling the drone}\label{sec:4_drone_ctrl}

\subsection{Connection}\label{subsec:4_drone_conn}
\subsection{Movement}\label{subsec:4_drone_mvmnt}
\subsection{GUI control panel}\label{subsec:4_drone_ctrl_panel_gui}
